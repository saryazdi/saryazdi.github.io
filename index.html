<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Soroush Saryazdi</title>
  
  <meta name="author" content="Soroush Saryazdi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/fav.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Soroush Saryazdi</name>
              </p>
              <p>I currently lead the Neural Networks team at <a href="https://matician.com">Matician</a>, supervised by the incredible 
                <a href="https://sites.google.com/view/navneetdalal/home">Navneet Dalal</a>.
              </p>
              <p> Recently my focus has been on: </br>
                  1. Building products that solve real problems & are easy to use </br>
                  2. Designing the perception system for an autonomous robot </br>
                  3. Developing accurate & reproducible on-device Vision Neural Networks
              </p>
              <p> I did my Computer Science Master's studies at <a href="https://www.concordia.ca">Concordia University</a> and in collaboration with <a href="https://liampaull.ca">Liam Paull</a>'s
                amazing <a href="https://montrealrobotics.ca">Robotics and Embodied AI Lab (REAL)</a> at <a href="https://mila.quebec">Mila</a>. I was focused on using end-to-end trainable models for recovering the 3D geometry and rendering properties of a scene.
              </p>
              <p>
                I sometimes review for NeurIPS, ICLR & ICML.
              </p>
              <p style="text-align:center">
                <a href="mailto:soroush.saryazdi@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/S_Saryazdi">Twitter</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=MjTlun4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/saryazdi">Github</a> &nbsp/&nbsp
                <a href="https://ca.linkedin.com/in/soroush-saryazdi-792365175">LinkedIn</a> &nbsp/&nbsp
                <a href="data/SoroushSaryazdi_CV.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SoroushSaryazdi.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/SoroushSaryazdi_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="80%" align="left" border="0" cellspacing="0" cellpadding="20" class="timeline" style="margin-left:20px"><tbody>
          <tr style="vertical-align:text-top">
            <td style="text-align:left"><strong>April</strong></td> 
            <td><strong>2022</strong></td>
            <td style="padding-bottom:5px;"><a href="https://concept-fusion.github.io">ConceptFusion</a> accepted at RSS 2023</td>
          </tr>
          <tr style="vertical-align:text-top">
            <td style="text-align:left"><strong>April</strong></td> 
            <td><strong>2022</strong></td>
            <td style="padding-bottom:5px;"><a href="https://iclr.cc/Conferences/2022/Reviewers">Highlighted reviewer</a> for ICLR 2022</td>
          </tr>
          <tr style="vertical-align:text-top">
            <td style="text-align:left"><strong>May</strong></td> 
            <td><strong>2021</strong></td>
            <td style="padding-bottom:5px;">Joined <a href="https://matician.com">Matician</a> as employee #30</td>
          </tr>
          <tr style="vertical-align:text-top">
            <td style="text-align:left"><strong>May</strong></td> 
            <td><strong>2021</strong></td>
            <td style="padding-bottom:5px;">Defended my Master's thesis & graduated</td>
          </tr>
          <tr style="vertical-align:text-top">
            <td style="text-align:left"><strong>April</strong></td> 
            <td><strong>2021</strong></td>
            <td style="padding-bottom:5px;">Our paper titled "Disentangled Rendering Loss for Supervised Material Property Recovery" won 
              the <a href="http://www.grapp.visigrapp.org/PreviousAwards.aspx">Best Student Paper Award</a> at GRAPP 2021.</td>
          </tr>
          <tr style="vertical-align:text-top">
            <td style="text-align:left"><strong>Nov.</strong></br></td> 
            <td><strong>2020</strong></br></td>
            <td style="padding-bottom:5px;">We released <a href="https://github.com/gradslam/gradslam">gradslam</a>: an open-source library for differentiable dense SLAM.</td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Featured Papers</heading>
              <p>
                Please see my <a href="https://scholar.google.com/citations?user=MjTlun4AAAAJ&hl=en">Google Scholar</a> for a complete list of papers.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="conceptfusion_stop()" onmouseover="conceptfusion_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='conceptfusion_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/conceptfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/conceptfusion_image.jpg' width="160">
              </div>
              <script type="text/javascript">
                function conceptfusion_start() {
                  document.getElementById('conceptfusion_image').style.opacity = "1";
                }

                function conceptfusion_stop() {
                  document.getElementById('conceptfusion_image').style.opacity = "0";
                }
                conceptfusion_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://concept-fusion.github.io">
                <papertitle>ConceptFusion: Open-set Multimodal 3D Mapping</papertitle>
              </a>
              <br>
              <a href="https://krrish94.github.io">Krishna Murthy Jatavallabhula</a>,
              <a href="https://www.alihkw.com/">Alihusein Kuwajerwala</a>*,
              <a href="https://georgegu1997.github.io/">Qiao Gu</a>*,
              <a href="https://scholar.google.com/citations?user=jFH3ShsAAAAJ&hl=en">Mohd Omama</a>*,
              <a href="https://taochenshh.github.io/">Tao Chen</a>,
              <a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>,
              <a href="https://epiception.github.io/">Ganesh Iyer</a>,
              <strong>Soroush Saryazdi</strong>,
              <a href="https://nik-v9.github.io/">Nikhil Keetha</a>,
              <a href="https://ayushtewari.com/">Ayush Tewari</a>,
              <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a>,
              <a href="https://celsodemelo.net/">Celso Miguel de Melo</a>,
              <a href="https://robotics.iiit.ac.in/">Madhava Krishna</a>,
              <a href="https://liampaull.ca">Liam Paull</a>,
              <a href="http://www.cs.toronto.edu/\~florian/">Florian Shkurti</a>,
              <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>
              <br>
              <em>RSS</em> 2023
              <br>
              <a href="https://concept-fusion.github.io">project page</a> /
              <a href="https://arxiv.org/pdf/2302.07241.pdf">paper</a> /
              <a href="data/ConceptFusion2023.bib">cite</a>
              <p></p>
              <p>ConceptFusion builds open-set 3D maps that can be queried via text, click, image, or audio.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='disentangledrenderingloss_image'>
                <img src='images/disentangledrenderingloss.png' width="160">
              </div>
            </td>            
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.scitepress.org/PublicationsDetail.aspx?ID=Z6fHuAhD1T8=&t=1">
              <papertitle>Disentangled Rendering Loss for Supervised Material Property Recovery</papertitle>
              </a>
              <br>
              <strong>Soroush Saryazdi</strong>,
              <a href="https://christiansmurphy.wixsite.com/myportfolio">Christian Murphy</a>,
              <a href="http://users.encs.concordia.ca/~mudur/">Sudhir P. Mudur</a>
              <br>
              <em>GRAPP</em> 2021 <font color="red"><strong>(Best Student Paper Award)</strong></font>
              <br>
              <a href="https://www.scitepress.org/Papers/2021/103303/103303.pdf">paper</a> /
              <a href="data/SaryazdiGRAPP2021.bib">cite</a>
              <p></p>
              <p>The disentangled rendering loss can be used in supervised material appearance estimation tasks for recovering more accurate individual BRDF parameters.</p>
            </td>
            </tr>

          <tr onmouseout="gradslam_stop()" onmouseover="gradslam_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gradslam_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/gradslam.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/gradslam_image.jpg' width="160">
              </div>
              <script type="text/javascript">
                function gradslam_start() {
                  document.getElementById('gradslam_image').style.opacity = "1";
                }

                function gradslam_stop() {
                  document.getElementById('gradslam_image').style.opacity = "0";
                }
                gradslam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://gradslam.github.io/">
                <papertitle><span>&#8711;</span>SLAM: Automagically differentiable SLAM</papertitle>
              </a>
              <br>
              <a href="https://krrish94.github.io">Krishna Murthy Jatavallabhula*</a>,
              <strong>Soroush Saryazdi*</strong>,
              <a href="https://epiception.github.io">Ganesh Iyer</a>,
              <a href="https://liampaull.ca">Liam Paull</a>
              <br>
              <em>CVPR workshop</em> 2020
              <br>
              <a href="https://gradslam.github.io">project page</a> /
              <a href="https://github.com/gradslam/gradslam">code</a> / 
              <a href="https://arxiv.org/abs/1910.10672">paper</a> /
              <a href="data/Gradslam2020.bib">cite</a>
              <p></p>
              <p><span>&#8711;</span>SLAM (gradSLAM) is a differentiable computational graph view of dense SLAM which allows gradients from SLAM outputs to be backpropagated to the input.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='entanglement_image'>
                <img src='images/mam2020.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://diglib.eg.org/handle/10.2312/mam20201138#:~:text=SVBRDF%20(spatially%20varying%20bidirectional%20reflectance,casual%20rather%20than%20calibrated%20captures.">
              <papertitle>The Problem of Entangled Material Properties in SVBRDF Recovery</papertitle>
              </a>
              <br>
              <strong>Soroush Saryazdi</strong>,
              <a href="https://christiansmurphy.wixsite.com/myportfolio">Christian Murphy</a>,
              <a href="http://users.encs.concordia.ca/~mudur/">Sudhir P. Mudur</a>
              <br>
              <em>Eurographics workshop</em> 2020
              <br>
              <a href="https://diglib.eg.org/bitstream/handle/10.2312/mam20201138/005-008.pdf">paper</a> /
              <a href="data/SaryazdiMAM2020.bib">cite</a>
              <p></p>
              <p>We analyze the landscape of the rendering loss and how it can negatively impact the network into estimating less accurate individual BRDF parameters.</p>
            </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template source code from <a href="http://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
